{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Data augmentation research** #"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import librosa\n","import soundfile as sf\n","import os\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow import keras\n","import shutil\n","import cv2\n","import matplotlib.pyplot as plt\n","import multiprocessing\n","import concurrent.futures\n","from time import perf_counter"]},{"cell_type":"markdown","metadata":{},"source":["**The following cell is for clearing the output directory from kaggle; run it only if needed**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Specify the path to the output directory\n","output_directory = \"/kaggle/working/\"\n","\n","# Iterate through the files and subdirectories in the output directory\n","for item in os.listdir(output_directory):\n","    item_path = os.path.join(output_directory, item)\n","    if os.path.isfile(item_path):\n","        os.remove(item_path)\n","    elif os.path.isdir(item_path):\n","        shutil.rmtree(item_path)\n","\n","print(\"Output directory cleared.\")"]},{"cell_type":"markdown","metadata":{},"source":["### Create distinct dataset\n","No overlap in songs with augmentated variants both in the train and test set\n","\n","NOTE: Code for generating N Segments per song was lost during an unfortunate incident. This method now assumes you want to make full spectrograms of the 30 seconds. This because the project had to proceed and after doing the experiments we had the results and wanted to continue with the full dataset anyway."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def prepare_dataset(base_path, test_size=10, validation_size=10):\n","    \"\"\"\n","    Prepares the dataset by selecting songs for each set (train, test, validation)\n","    without moving or copying files.\n","    \"\"\"\n","    dataset_info = {'train': [], 'test': [], 'validation': []}\n","\n","    for genre_folder in os.listdir(base_path):\n","        genre_path = os.path.join(base_path, genre_folder)\n","        if os.path.isdir(genre_path):\n","            all_songs = os.listdir(genre_path)\n","            np.random.shuffle(all_songs)\n","\n","            test_songs = all_songs[:test_size]\n","            validation_songs = all_songs[test_size:test_size + validation_size]\n","            train_songs = all_songs[test_size + validation_size:]\n","\n","            for song in test_songs:\n","                dataset_info['test'].append((os.path.join(genre_path, song), genre_folder))\n","            for song in validation_songs:\n","                dataset_info['validation'].append((os.path.join(genre_path, song), genre_folder))\n","            for song in train_songs:\n","                dataset_info['train'].append((os.path.join(genre_path, song), genre_folder))\n","\n","    return dataset_info\n","\n","def generate_spectrogram(file_path, genre, output_path):\n","    \"\"\"\n","    Generates and saves a spectrogram for a given audio file.\n","    \"\"\"\n","    y, sr = librosa.load(file_path)\n","    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n","    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n","\n","#     plt.figure(figsize=(10, 4))\n","    librosa.display.specshow(spectrogram_db)\n","    \n","    song_name = os.path.basename(file_path)\n","    # Split the string on the period (.)\n","    split_words = song_name.split('.')\n","    # Take the first two words and concatenate them with an underscore\n","    song_name = '.'.join(split_words[:2])\n","    \n","    output_genre_path = os.path.join(output_path, genre)\n","    if not os.path.exists(output_genre_path):\n","        os.makedirs(output_genre_path)\n","    plt.savefig(os.path.join(output_genre_path, f'{song_name}.png'))\n","    plt.close()\n","\n","def process_songs_parallel(dataset_info, output_path):\n","    \"\"\"\n","    Processes the songs in parallel using multiprocessing to generate spectrograms.\n","    \"\"\"\n","    with multiprocessing.Pool() as pool:\n","        for set_name, songs_info in dataset_info.items():\n","            for song_path, genre in songs_info:\n","                if set_name == \"train\":\n","                    continue\n","                else:\n","                    pool.apply_async(generate_spectrogram, args=(song_path, genre, os.path.join(output_path, set_name)))\n","        pool.close()\n","        pool.join()\n","\n","# Usage:\n","base_path = \"/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original\"\n","spectrogram_output_path = \"/kaggle/working/spectrograms_computed\"\n","dataset_info = prepare_dataset(base_path)\n","process_songs_parallel(dataset_info, spectrogram_output_path)"]},{"cell_type":"markdown","metadata":{},"source":["### Create train set\n","generate the spectrograms of te train set audio"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["'Define the function to compute and save the spectograms'\n","\n","def spectrogram_creation(audio_path, song_name, spectrogram_save_path, num_augment=3):\n","    \"\"\"\n","    Create a spectrogram of a song. This will also create augmented versions of your dataset when \"num_segment\" > 1\n","    \"\"\"\n","    # Load audio file\n","    y, sr = librosa.load(audio_path)\n","\n","    # Create Figure and Axes objects\n","#     fig, ax = plt.subplots(figsize=(10, 4))\n","\n","    for i in range(num_augment):\n","        \n","        # Apply augmentation with a certain probability\n","        if i > 0:\n","            spectrogram_save_path = spectrogram_save_path + f\"/Aug_\"\n","            y = apply_random_augmentation(y, sr)\n","        else:\n","            spectrogram_save_path = spectrogram_save_path + f\"/\"\n","\n","        # Compute the spectrogram and convert to dB\n","        spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n","        spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n","\n","        # Plot the spectrogram on the existing Axes\n","        librosa.display.specshow(spectrogram_db)\n","\n","        # Save the figure to the file\n","        plt.savefig(f'{spectrogram_save_path}-{song_name}.png')\n","\n","        # Clear the existing plot for the next iteration\n","#         ax.cla()\n","\n","    # Close the Figure to release resources\n","    plt.close()\n","\n","# Choose a random data augmentation method to apply on the signal\n","def apply_random_augmentation(signal, sr):\n","    # Randomly choose an augmentation function or return the original signal\n","    augmentation_functions = [add_white_noise, pitch_scale, random_gain]\n","    chosen_function = np.random.choice(augmentation_functions)\n","\n","    # Apply the chosen augmentation function\n","    augmented_signal = chosen_function(signal, sr)\n","\n","    return augmented_signal\n","\n","def add_white_noise(signal, sr, noise_percentage_factor=0.005):\n","    noise = np.random.normal(0, signal.std(), signal.size)\n","    augmented_signal = signal + noise * noise_percentage_factor\n","    return augmented_signal\n","\n","def pitch_scale(signal, sr, num_semitones=2):\n","    return librosa.effects.pitch_shift(y=signal, sr=sr, n_steps=num_semitones)\n","\n","def random_gain(signal, sr, min_factor=0.8, max_factor=1.2):\n","    gain_rate = np.random.uniform(min_factor, max_factor)\n","    augmented_signal = signal * gain_rate\n","    return augmented_signal\n"]},{"cell_type":"markdown","metadata":{},"source":["## Code to actually process the train set \n","Calls the parallelized routines to convert audio files to spectrograms and create the augmentations if needed (based on 30 second full clips)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def process_genre_group(genre_group, output_base_path):\n","    \"\"\"\n","    Process a group of songs of the same genre.\n","    :param genre_group: List of tuples (song_path, genre).\n","    :param output_base_path: Base output path for saving spectrograms.\n","    \"\"\"\n","    for song_path, genre in genre_group:\n","        worker(song_path, genre, output_base_path)\n","\n","def worker(song_path, genre, output_base_path):\n","    song_name = os.path.basename(song_path)\n","    # Split the string on the period (.)\n","    split_words = song_name.split('.')\n","    # Take the first two words and concatenate them with an underscore\n","    song_name = '.'.join(split_words[:2])\n","    \n","    genre_folder_path = os.path.join(output_base_path, 'train', genre)\n","    if not os.path.exists(genre_folder_path):\n","        os.makedirs(genre_folder_path)\n","\n","    spectrogram_creation(song_path, song_name, genre_folder_path)\n","\n","def process_training_set_parallel(training_songs_info, output_path):\n","    \"\"\"\n","    Processes the training set songs in parallel using multiprocessing,\n","    with each genre processed in a different process.\n","\n","    :param training_songs_info: List of tuples containing song paths and their genres.\n","    :param output_path: Path where the training set spectrograms will be saved.\n","    \"\"\"\n","    training_folder_path = os.path.join(output_path, 'train')\n","    if not os.path.exists(training_folder_path):\n","        os.makedirs(training_folder_path)\n","\n","    # Group songs by genre\n","    genre_groups = {}\n","    for song_path, genre in training_songs_info:\n","        if genre not in genre_groups:\n","            genre_groups[genre] = []\n","        genre_groups[genre].append((song_path, genre))\n","        \n","#     print(genre_groups)\n","\n","    # Process each genre group in a separate process\n","    with multiprocessing.Pool() as pool:\n","        for genre_group in genre_groups.values():\n","#             print(genre_group)\n","            pool.apply_async(process_genre_group, args=(genre_group, output_path))\n","        pool.close()\n","        pool.join()\n","        \n","        \n","# Assuming you have the dataset_info dictionary from prepare_dataset function\n","training_songs_info = dataset_info['train']\n","process_training_set_parallel(training_songs_info, spectrogram_output_path)"]},{"cell_type":"markdown","metadata":{},"source":["One audios from the dataset is not used: \n","\n","* one audio that cannot be loaded neither with librosa or soundfile libraries.\n"]},{"cell_type":"markdown","metadata":{},"source":["**Check if the number of files from each directory is Correct**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the parent directory you want to analyze\n","parent_directory = '/kaggle/working/spectrograms_computed/train'\n","# parent_directory = \"/kaggle/input/gtzan-dataset-music-genre-classification/Data/images_original\"\n","\n","# Create an empty dictionary to store directory names and file counts\n","directory_file_counts = {}\n","\n","# Walk through the parent directory and count files in each subdirectory\n","for root, dirs, files in os.walk(parent_directory):\n","    # Count the files in the current directory\n","    file_count = len(files)\n","    \n","    # Store the directory name and file count in the dictionary\n","    directory_file_counts[root] = file_count\n","\n","# Print the directory names and their respective file counts\n","for directory, file_count in directory_file_counts.items():\n","    print(f\"Directory: {directory}, File Count: {file_count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["classes = [a for a in os.listdir('/kaggle/input/gtzan-dataset-music-genre-classification/Data/images_original') if '.' not in a]\n","print(classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def process_dataset(folder_path, classes, img_size=256):\n","    \"\"\"\n","    Processes images in the given folder into a dataset.\n","\n","    :param folder_path: Path to the folder containing images.\n","    :param classes: List of class names.\n","    :param img_size: Size to which images are resized.\n","    :return: Tuple of numpy arrays (features, labels).\n","    \"\"\"\n","    dataset = []\n","\n","    for label in classes:\n","        class_path = os.path.join(folder_path, label)\n","        class_num = classes.index(label)\n","\n","        for img in os.listdir(class_path):\n","            try:\n","                img_arr = cv2.imread(os.path.join(class_path, img))[...,::-1]  # Convert BGR to RGB\n","                resized_arr = cv2.resize(img_arr, (img_size, img_size))  # Reshape images\n","                dataset.append([resized_arr, class_num])\n","            except Exception as e:\n","                print(f\"Error processing {img}: {e}\")\n","\n","    features, labels = zip(*dataset)\n","    return np.array(features), np.array(labels)\n","\n","base_directory = \"/kaggle/working/spectrograms_computed\"\n","\n","# Process each dataset\n","x_train, y_train = process_dataset(os.path.join(base_directory, 'train'), classes)\n","x_test, y_test = process_dataset(os.path.join(base_directory, 'test'), classes)\n","x_valid, y_valid = process_dataset(os.path.join(base_directory, 'validation'), classes)\n","print(\"Done...\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x_train = x_train / 255\n","x_test = x_test / 255\n","x_valid = x_valid / 255\n","print(x_train.shape, y_train.shape)\n","print(x_test.shape, y_test.shape)\n","print(x_valid.shape, y_valid.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# **Transfer learning part** #"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import applications, layers, models, regularizers\n","from tensorflow.keras.optimizers import Adam\n","from keras.applications.vgg16 import VGG16\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout\n","from tensorflow.keras.callbacks import EarlyStopping"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = Sequential()\n","model.add(Conv2D(32,3,padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.01), input_shape=(256,256,3), kernel_regularizer='l2'))\n","model.add(MaxPool2D())\n","\n","model.add(Conv2D(32, 3, padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer='l2'))\n","model.add(MaxPool2D())\n","\n","model.add(Conv2D(64, 3, padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer='l2'))\n","model.add(MaxPool2D())\n","\n","model.add(Conv2D(64, 3, padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer='l2'))\n","model.add(MaxPool2D())\n","\n","model.add(Flatten())\n","model.add(Dense(128,activation=tf.keras.layers.LeakyReLU(alpha=0.01)))\n","model.add(Dropout(0.2))\n","model.add(Dense(10, activation=\"softmax\"))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Fit the model\n","model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=40, batch_size=16)\n","\n","# Evaluate the model on the test set\n","results = model.evaluate(x_test, y_test)\n","\n","# Print the evaluation results\n","print(\"Test Loss:\", results[0])\n","print(\"Test Accuracy:\", results[1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load the model chose for transfer learning, excluding the top (classification) layer\n","base_model =  tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n","\n","for layer in base_model.layers:\n","    layer.trainable = False\n","  \n","    \n","x = base_model.output \n","# Add your custom classification head\n","x = keras.layers.GlobalAveragePooling2D()(x)\n","x = layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.005))(x)\n","x = layers.Dropout(0.2)(x)\n","output = layers.Dense(10, activation='softmax')(x)  # 10 classes for music genres\n","\n","# Create a new model with the custom input and classification head\n","model = models.Model(inputs=base_model.input, outputs=output)\n","\n","# model.summary()\n","\n","# Compile the model\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# callbacks = [EarlyStopping(monitor='val_loss', patience=7, verbose=1)]\n","\n","# Fit the model\n","model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=30, batch_size=32)\n","\n","# Evaluate the model on the test set\n","results = model.evaluate(x_test, y_test)\n","\n","# Print the evaluation results\n","print(\"Test Loss:\", results[0])\n","print(\"Test Accuracy:\", results[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.applications import Xception\n","\n","base_model = Xception(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n","# Freeze the layers of the pre-trained model\n","for layer in base_model.layers:\n","    layer.trainable = False\n","    \n","base_model.layers[-1].trainable = True\n","\n","# Define the custom classification head\n","x = base_model.output\n","x = keras.layers.GlobalAveragePooling2D()(x)\n","x = layers.Dense(128, activation='relu')(x)\n","x = layers.Dropout(0.2)(x)\n","output = layers.Dense(10, activation='softmax')(x)  # 10 classes for music genres\n","\n","# Create a new model with the custom input and classification head\n","model = models.Model(inputs=base_model.input, outputs=output)\n","\n","# Compile the model\n","model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.01, momentum = 0.7), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Fit the model\n","model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=50, batch_size=8)\n","\n","# Evaluate the model on the test set\n","results = model.evaluate(x_test, y_test)\n","# Print the evaluation results\n","print(\"Test Loss:\", results[0])\n","print(\"Test Accuracy:\", results[1])"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":568973,"sourceId":1032238,"sourceType":"datasetVersion"},{"datasetId":3940502,"sourceId":6855464,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
